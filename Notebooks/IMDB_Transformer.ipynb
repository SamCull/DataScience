{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90145b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf294a",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b><span style = \"color:#000000\">\n",
    " Load the Preprocess the Dataset </span> </b>\n",
    "    \n",
    "* The IMDb dataset is loaded using pandas.read_csv. <br><br>\n",
    "    \n",
    "* HTML tags are removed from the reviews to clean the text data. <br><br>\n",
    "\n",
    "* The dataset size is adjusted for quicker training and testing, using a small sample (test_size=0.02). <br><br>\n",
    "    \n",
    "* This small sample is then split into training and testing sets. <br><br>\n",
    "    \n",
    "--------------\n",
    "-----\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "127807fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "# Simple preprocessing: removing possible HTML tags\n",
    "df['review'] = df['review'].str.replace('<.*?>', ' ')\n",
    "\n",
    "# Adjust the dataset size for quicker training and testing\n",
    "_, sample_data, _, sample_labels = train_test_split(df['review'], df['sentiment'], test_size=0.02, random_state=42)\n",
    "\n",
    "# split the small sample into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(sample_data, sample_labels, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cf271",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b><span style = \"color:#000000\">\n",
    " Text Tokenization and Padding </span> </b>\n",
    "    \n",
    "* The Tokenizer is configured to only consider the top 10,000 words. <br><br>\n",
    "    \n",
    "* Text data (reviews) is converted into sequences of integers, where each integer represents a specific word-token. <br><br>\n",
    "\n",
    "* Sequences are padded to ensure they have the same length for model input, using pad_sequences. <br>\n",
    "        \n",
    "--------------\n",
    "-----\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e00400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Padding sequences to ensure uniform input size\n",
    "max_length = max(max(len(x) for x in train_sequences), max(len(x) for x in test_sequences))\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6f952",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b><span style = \"color:#000000\">\n",
    " Building the Transformer Model </span> </b>\n",
    "    \n",
    "* Defines a transformer_encoder function that includes a multi-head self-attention mechanism and \n",
    "a feed-forward network, fundamental components of a transformer. <br><br>\n",
    "    \n",
    "* Constructs the neural network model with an embedding layer, followed by the transformer encoder and global average pooling, culminating in a dense layer for binary classification (positive or negative sentiment). <br><br>\n",
    "\n",
    "* The model is compiled with the Adam optimizer and binary cross-entropy loss, suitable for binary classification tasks. <br>\n",
    "        \n",
    "--------------\n",
    "-----\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef8e1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def transformer_encoder(inputs, num_heads, ff_dim):\n",
    "    # Multi-head self-attention\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(0.1)(attention_output)\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward layer\n",
    "    ff_output = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(inputs.shape[-1])\n",
    "    ])(attention_output)\n",
    "    ff_output = tf.keras.layers.Dropout(0.1)(ff_output)\n",
    "    return tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + ff_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20114b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 1007)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 1007, 64)             640000    ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 1007, 64)             16640     ['embedding_3[0][0]',         \n",
      " ltiHeadAttention)                                                   'embedding_3[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 1007, 64)             0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 1007, 64)             0         ['embedding_3[0][0]',         \n",
      " OpLambda)                                                           'dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 1007, 64)             128       ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 1007, 64)             4192      ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 1007, 64)             0         ['sequential_3[0][0]']        \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 1007, 64)             0         ['layer_normalization_6[0][0]'\n",
      " OpLambda)                                                          , 'dropout_7[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 1007, 64)             128       ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 64)                   0         ['layer_normalization_7[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1)                    65        ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 661153 (2.52 MB)\n",
      "Trainable params: 661153 (2.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "inputs = tf.keras.layers.Input(shape=(max_length,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=10000, output_dim=64)(inputs)\n",
    "transformer_block = transformer_encoder(embedding_layer, num_heads=2, ff_dim=32)\n",
    "global_average_pooling = tf.keras.layers.GlobalAveragePooling1D()(transformer_block)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(global_average_pooling)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150926f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b><span style = \"color:#000000\">\n",
    " Training the Model </span> </b>\n",
    "    \n",
    "* Converts sentiment labels (positive/negative) into numeric format (1/0) for model training. <br><br>\n",
    "    \n",
    "* Trains the model on the padded training sequences with corresponding labels, using a portion of the data for validation. <br><br>\n",
    "--------------\n",
    "-----\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fb4a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 48s 3s/step - loss: 0.7478 - accuracy: 0.4960 - val_loss: 0.6853 - val_accuracy: 0.6260\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.6665 - accuracy: 0.5780 - val_loss: 0.6806 - val_accuracy: 0.5300\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 41s 3s/step - loss: 0.6341 - accuracy: 0.6180 - val_loss: 0.6634 - val_accuracy: 0.6340\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 42s 3s/step - loss: 0.6025 - accuracy: 0.7280 - val_loss: 0.6486 - val_accuracy: 0.7060\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.5475 - accuracy: 0.8880 - val_loss: 0.6309 - val_accuracy: 0.7100\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.5241 - accuracy: 0.7360 - val_loss: 0.6559 - val_accuracy: 0.5560\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.4738 - accuracy: 0.7400 - val_loss: 0.6295 - val_accuracy: 0.6060\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.3867 - accuracy: 0.8160 - val_loss: 0.5729 - val_accuracy: 0.6740\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 37s 2s/step - loss: 0.1951 - accuracy: 0.9660 - val_loss: 0.5101 - val_accuracy: 0.7580\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.0293 - accuracy: 0.9960 - val_loss: 0.6466 - val_accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numeric\n",
    "train_labels = train_labels.replace({'positive': 1, 'negative': 0})\n",
    "test_labels = test_labels.replace({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3f476",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b><span style = \"color:#000000\">\n",
    " Evaluating Model and making Predictions </span> </b>\n",
    "    \n",
    "* Evaluates the trained model on the test set to obtain loss and accuracy metrics, providing insights into its performance. <br><br>\n",
    "    \n",
    "* Demonstrates making a prediction with the model on a new sample text, showcasing its practical application. <br><br>\n",
    "--------------\n",
    "-----\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c3dab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 13s 832ms/step - loss: 0.6466 - accuracy: 0.7700\n",
      "Test Loss: 0.6465569138526917, Test Accuracy: 0.7699999809265137\n",
      "1/1 [==============================] - 1s 707ms/step\n",
      "Sentiment Prediction: Positive\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_padded, test_labels)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n",
    "\n",
    "# Making a prediction\n",
    "sample_text = [\"This movie was a great journey full of emotion and excitement\"]\n",
    "sample_sequence = tokenizer.texts_to_sequences(sample_text)\n",
    "sample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post')\n",
    "prediction = model.predict(sample_padded)\n",
    "print(f'Sentiment Prediction: {\"Positive\" if prediction[0] > 0.5 else \"Negative\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cfa52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
